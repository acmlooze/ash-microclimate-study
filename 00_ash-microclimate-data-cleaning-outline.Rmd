---
title: "ash-microclimate-data-cleaning"
author: "Deidre Jaeger"
date: "10/12/2018"
output: html_document
---

Get that data Tidy!

*Step 1:* import the treatment sensor datafiles
  - upon import: skip the header rows, rename columns
  - assign sensor label and ash removal status
  - format the date and time in R friendly POSIXct format
*Step 2:* import the control sensor datafiles
  - upon import: skip the header rows, rename columns
  - format the date and time in R friendly POSIXct format
  - assign sensor label and ash removal status

*Step 3:* bind all the csvs from each sensor into 1 single dataframe
   - each row is temperature measurement at a given second at a given location spanning the entire study          period (Sept 18 to Sept 22, 2018)
*Step 4:* set factors for categorical name and serial number of sensors and ash removal status
*Step 5:* export a new csv with the cleaned data
*Step 6:* fill in a textfile called "readme-boulder-ash-data2018" with the metadata about the sensor data
            - include: data about each of the trees and locations, the types of sensors used, the type of radiation shield used, the height placed from the ground


 
```{r load-libraries}
# use install.packages("insert-package-name-here") if you get an error message that says "there is no package called ....x...."

library(dplyr) # for mutate() functionality
library(lubridate) # for as.POSIX function
options(stringsAsFactors = FALSE) # so that dates read in as character strings not factors (factors are troublesome for converting to dates)
```
 
```{r step1-import-treatment-data}

# import the data from the treatment and control sensors

# open the first file of the treatment-tree 100e, from the "before" removal
treatment_100e_before <- read.csv("data/SN 20439121 2018-09-20 00_06_33 -0600.csv", # define the file name
                          skip = 1, # skip reading the first row
                          header = TRUE, # define the second row as the header
                          col.names = c("date_time", "tempF", "sensor_name", "ash_removal", "blank1",  "blank2")) # rename the columns

treatment_100e_before$blank1 <- NULL # Remove the blank columns
treatment_100e_before$sensor_sn <- rep(20439121) # add the sensor serial number
treatment_100e_before$sensor_name <- rep("treatment_100e") # fill column with the sensor name
treatment_100e_before$ash_removal <- rep("before_removal") # fill the column with the ash removal status


# check the first 6 rows to see if the dataframe looks as we expect
head(treatment_100e_before)

# check the structure of how it is loaded
str(treatment_100e_before) # we will eventually want date to be formated as date, but for now having number for temp and character for sensor name look good


# open the second file of the treatment-tree 100e, from the "after" removal
# open the first file of the treatment-tree 100e, from the "before" removal
treatment_100e_after <- read.csv("data/SN 20439121 2018-09-26 10_27_52 -0600.csv", # define the file name
                          skip = 1, # skip reading the first row
                          header = TRUE, # define the second row as the header
                          col.names = c("date_time", "tempF", "sensor_sn","sensor_name", "ash_removal", "blank1")) # rename the columns
treatment_100e_after$blank1 <- NULL # Remove the blank columns
treatment_100e_after$sensor_sn <- rep(20439121) # add the sensor serial number
treatment_100e_after$sensor_name <- rep("treatment_100e") # fill column with the sensor name
treatment_100e_after$ash_removal <- rep("after_removal") # fill the column with the ash removal status

# check the first 6 rows to see if the dataframe looks as we expect
head(treatment_100e_after)

# combine the before and after treatment into 1 dateframe
treatment_100e_clean <- rbind(treatment_100e_before, treatment_100e_after)

# check the structure of how it is loaded
str(treatment_100e_clean) # we will eventually want date to be formated as date

# format 
treatment_100e_clean <- treatment_100e_clean %>% 
  mutate(date_time = ymd_hms(date_time)) # format date in R-friendly format

str(treatment_100e_clean) # check that class structure changes to "POSIXct"

```


```{r step2-import-control-data-files}
# open the first file of the treatment-tree 100e, from the "before" removal
control_103_data <- read.csv("data/SN 20397886 2018-09-26 10_27_32 -0600.csv", # define the file name
                          skip = 1, # skip reading the first row
                          header = TRUE, # define the second row as the header
                          col.names = c("date_time", "tempF", "sensor_sn","sensor_name", "ash_removal", "blank1")) # rename the columns

control_103_data$blank1 <- NULL # Remove the blank columns
control_103_data$sensor_sn <- rep(20397886) # add the sensor serial number
control_103_data$sensor_name <- rep("control_103") # fill column with the sensor name

# format date/time
control_103_data <- control_103_data %>% 
  mutate(date_time = ymd_hms(date_time)) # format date in R-friendly format

# assign "before" ash_removal categories
control_103_data_before <- control_103_data %>% 
  filter(date_time <= ymd_hms("2018-09-20 00:03:59")) %>% 
  mutate(ash_removal = "before_removal")

# assign "during" ash_removal categories ( Deidre needs to fix this still!)
control_103_data_during <- control_103_data %>% 
  filter((date_time >= ymd_hms("2018-09-20 00:04:00")) & (date_time <= ymd_hms("2018-09-20 12:20:59"))) %>% 
  mutate(ash_removal = "during_removal")

# assign "after" ash_removal categories
control_103_data_after <- control_103_data %>% 
  filter(date_time >= ymd_hms("2018-09-20 12:21:00")) %>% 
  mutate(ash_removal = "after_removal")

# zip back up into 1 dataframe
control_103_data_clean <- rbind(control_103_data_before, control_103_data_during, control_103_data_after)

# check that all rows were assigned an ash removal status
unique(control_103_data_clean$ash_removal)# Yay! There are no NA's meaning we captured all the rows
nrow(control_103_data) == nrow(control_103_data_clean) # and we didn't lose any rows!
```



# could create a loop to do these cleaning steps for all the files in the temp data folder
# dataframe names:
treatment_100e_before (example above)
treatment_100e_after (example above)
treatment_101w_before
treatment_101w_after
control_103_before 
control_103_after 
control_sun_before 
control_sun_after 

```{r step3-bind-into-1-csv}

# aggregate the before and after dataframes for all sensor data into 1 dataframe (best practice for tidy analyses)

# for example:
all_data <- rbind(treatment_100e_clean, control_103_data_clean)

```
 
```{r step4-format-date-time}
# we want the sensor serial number, sensor name, and the ash removal status columns to be categorical factors, rather than text characters

# for example:
all_data <- all_data %>% 
  mutate(sensor_sn = as.factor(sensor_sn), sensor_name = as.factor(sensor_name), ash_removal = as.factor(ash_removal))

str(all_data) # check that it worked
```

```{r step5-set-factors}

# check for NAs in dataset
all_data %>% 
  filter(is.na(tempF)) # seven rows don't have a temp value

# remove the rows that have NA temperature values with (! is a boolean operator and means "is not equal to"
all_data <- all_data %>% 
  filter(!is.na(tempF))
```

```{r step6-export-csv}
# export this new dataframe as "ash-microclimate-data2018" so we don't have to rebuild it each time we want to work with it, or we can send it someone over email

# for example: 
all_data %>% 
   write.csv(file="data/ash-microclimate-data2018.csv") # using pipes, write a new csv file saved at the defined filepath
  
```






Example function code from my accelerometer data cleaning to modify for temp sensors

```{r-example-for-loop-data-loading}

clean_up_raw_accel_csv <- function(files, output_path) {
  # This function will loop through a folder of raw Gulf Data Concepts Accelerometer Data csv files and produce a cleaned up and simplified csv that has columns: Seconds Elapsed, Ax, Ay, and Az acceleration axis, and the starting file timestamp
  ## Inputs are: 1) a list of files within a folder and 2) a path for the folder to which the new outputs will be stored
  ## Outputs are: the new csv files named with the site name and the original file number
  
  for (f in (1:(length(files)))) {
    site_meta <- read.csv(files[f], 
                             header= TRUE, # allow a header row
                             skip = 1, # skip the first row
                             col.names = c("Item", "Date", "Time", "blank1", "blank2"),
                             nrows = 1) # stop reading data after 1 row
    site_meta$blank1 <- NULL # Remove the null column
    site_meta$blank2 <- NULL # Remove the null column
    
    site_acc <- read.csv(files[f], # specify the file
                            skip = 7, # skip the first 7 rows
                            header = TRUE,
                            col.names = c("sec_elapse", "Ax", "Ay", "Az", "blank1"))
    site_acc$blank1 <- NULL # Remove the null column
    
    # reformat date to class date
    site_meta <- site_meta %>% 
      mutate(Date = as.Date(Date, format = "%Y-%m-%d")) %>% # reformat date
      mutate(day_time = paste(Date, Time)) %>% 
      mutate(day_time = as.POSIXct(day_time, format = "%Y-%m-%d %H:%M:%S"))
    
    # pull out the date
    site_date <- site_meta$Date
    # pull out the starting time
    site_start_time <- site_meta$day_time
    
    # parse out variable names
    csv_file_number <- gsub(".*-|\\.csv$", "", files[f])
    csv_site_name <- gsub("(.*Fall2017/)", "", files[f]) # keep everything after Fall 2017 in path name
    csv_site_name <- gsub("(/GCDC.*)", "", csv_site_name) 
    
    # Add Date and starting time column to site_acc, 
    site_acc <- site_acc %>% 
      mutate(Time = site_start_time) %>% 
      # write a new csv file as an output into a new MAIN folder
      write.csv(file = paste0(output_path, csv_site_name,"-", csv_file_number)) # using pipes, write a new csv file saved at the defined filepath
      print(paste0(csv_site_name,"-", csv_file_number, " created"))
  }
}

```
 
```{r-use-function-data-loading}
# Run function for each of the sites:

##### MAIN1
main1_files <- list.files("/Volumes/BIG_DATA1/Accelerometer/Fall2017/MAIN1/GCDC/",
                         full.names = TRUE)
main1_path = "/Users/deidrejaeger/Documents/Career/CU-Boulder/Research/BoulderPhenology/Accelerometers/Fall2017_study/outputs/main1/"
clean_up_raw_accel_csv(main1_files, main1_path)
```

 